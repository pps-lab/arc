$SUITE_VARS$:
  sleep_time: 2.0 # TODO [hly] can remove (also from experiment runner) as it does not have any effect

  _python_pre: "export PYTHONPATH={{ exp_code_dir }}/utils && . {{ exp_code_dir }}/.venv/bin/activate"
  _python_path: "[% my_run._python_pre %] && {{ exp_code_dir }}/.venv/bin/python"
  cmd_mpspdz: "[% my_run._python_path %] -m  python_utils.scripts.experiment_runner --player-number <PLAYER-ID> --sleep-time [% my_run.sleep_time %]"
#
#  _player_input_binary_path: "{{ mpspdz_dir }}/Player-Data/Input-Binary-P0-0"
#  cmd_gen_commitments: "{{ exp_consistency_dir }}/target/release/gen_commitments_[% my_run.compute_args.pc %] --hosts {{ exp_consistency_hosts_file }} --party <PLAYER-ID> --player-input-binary-path [% my_run._player_input_binary_path %] --save"
#
#  _stdout_path: "stdout.log"
#  cmd_prove_verify: "{{ exp_consistency_dir }}/target/release/prove_verify_[% my_run.compute_args.pc %] --hosts {{ exp_consistency_hosts_file }} --party <PLAYER-ID> --mpspdz-output-file [% my_run._stdout_path %]"

two:
  n_repetitions: 1
  host_types:
    consistency:
      n: 2
      check_status: True
      init_roles:
        - setup-base
        - setup-consistency
        - download-dataset
#        - setup-largeprimes
#        - setup-network-delay # adjust latency + bandwidth limit -> doe-suite-config/roles/setup-network-delay/vars/main.yml
      $CMD$:
      - main: "[% my_run.cmd_mpspdz | replace('<PLAYER-ID>', 0) %]"
      - main: "[% my_run.cmd_mpspdz | replace('<PLAYER-ID>', 1) %]"

  base_experiment:

    consistency_args:
      abs_path_to_code_dir: "{{ exp_consistency_dir }}"
      hosts_file: "{{ exp_consistency_hosts_file }}"
      pc: kzg
      pp_args: 1796300 # max of input sizes we need

    mpc:
      player_0_hostname: "[% exp_host_lst | json_query('[?host_type==`consistency`].private_dns_name') | default(['<UNDEFINED-DNS>'], true) | first %]"
      abs_path_to_code_dir: "{{ exp_code_dir }}"
      player_count: 2
      protocol_setup: "lowgear-party"

      compiler_args: $FACTOR$ #'100000000'] # budget was '1000000'1million # also look at B 5 -> cannot be used together with R
      domain: $FACTOR$
      custom_prime: $FACTOR$
      custom_prime_length: $FACTOR$

      script_name: "audit_prediction_shap"

      script_args:
        # inference specific
#        n_samples: 262000
        n_input_parties: 2

        debug: True  # for a final benchmarking run, can also use `debug: False`, this should then only release no secret info
        emulate: False
        dataset: $FACTOR$

        # in a network with latency, a high batch size for ml.py is key to a good performance (60k raised insufficient memory error)
        batch_size: 1024 #1024 #256 # was 128 before   # 60000 -> insufficient memory
        n_threads: 32 #128 #64 # TODO before was 32 -> see if this helps (128 raises too many files runtime error)

        audit_trigger_idx: 0 # select single audit trigger (from all)


      stage: # "compile" in run and then in next run "run"
        $FACTOR$: [ compile, run ] # ensure that this is last factor
  factor_levels:
    - mpc:
        domain: custom_256
        custom_prime: '8444461749428370424248824938781546531375899335154063827935233455917409239041'
        custom_prime_length: null
        compiler_args: [ '-F 251', '-C', '--budget', '10000', '-Y']
        script_args:
          dataset: "adult"
    - mpc:
        domain: prime_128
        compiler_args: [ '-C', '--budget', '10000', '-Y' ]
        custom_prime: null
        custom_prime_length: null
        script_args:
          dataset: "adult"
    - mpc:
        domain: prime_256
        compiler_args: [ '-F', "256", '-C', '--budget', '10000', '-Y' ]
        custom_prime: null
        custom_prime_length: '256'
        script_args:
          dataset: "adult"

three:
  n_repetitions: 1
  host_types:
    consistency:
      n: 3
      check_status: True
      init_roles:
        - setup-base
        - setup-consistency
#        - download-dataset
#        - setup-spdz-secrets
      #        - setup-largeprimes
      #        - setup-network-delay # adjust latency + bandwidth limit -> doe-suite-config/roles/setup-network-delay/vars/main.yml
      $CMD$:
        - main: "[% my_run.cmd_mpspdz | replace('<PLAYER-ID>', 0) %]"
        - main: "[% my_run.cmd_mpspdz | replace('<PLAYER-ID>', 1) %]"
        - main: "[% my_run.cmd_mpspdz | replace('<PLAYER-ID>', 2) %]"

  base_experiment:

    consistency_args:
      abs_path_to_code_dir: "{{ exp_consistency_dir }}"
      hosts_file: "{{ exp_consistency_hosts_file }}"
      pc: kzg
      pp_args: 1796300 # max of input sizes we need

    mpc:
      player_0_hostname: "[% exp_host_lst | json_query('[?host_type==`consistency`].private_dns_name') | default(['<UNDEFINED-DNS>'], true) | first %]"
      abs_path_to_code_dir: "{{ exp_code_dir }}"
      player_count: 3
      protocol_setup: "shamir_malicious_n"

      compiler_args: $FACTOR$ #'100000000'] # budget was '1000000'1million # also look at B 5 -> cannot be used together with R
      domain: $FACTOR$
      custom_prime: $FACTOR$
      custom_prime_length: $FACTOR$

      script_name: "audit_prediction_shap"

      script_args:
        # inference specific
        #        n_samples: 262000
        n_input_parties: 3

        debug: True  # for a final benchmarking run, can also use `debug: False`, this should then only release no secret info
        emulate: False
        dataset: $FACTOR$

        # in a network with latency, a high batch size for ml.py is key to a good performance (60k raised insufficient memory error)
        batch_size: 1024 #1024 #256 # was 128 before   # 60000 -> insufficient memory
        n_threads: 32 #128 #64 # TODO before was 32 -> see if this helps (128 raises too many files runtime error)

        audit_trigger_idx: 0 # select single audit trigger (from all)


      stage: # "compile" in run and then in next run "run"
        $FACTOR$: [ compile, run ] # ensure that this is last factor
  factor_levels:
    - mpc:
        domain: prime_128
        compiler_args: [ '-C', '--budget', '10000', '-Y' ]
        custom_prime: null
        custom_prime_length: null
        script_args:
          dataset: "adult"
    - mpc:
        domain: prime_256
        compiler_args: [ '-F', "256", '-C', '--budget', '10000', '-Y' ]
        custom_prime: null
        custom_prime_length: '256'
        script_args:
          dataset: "adult"
    - mpc:
        domain: custom_256
        custom_prime: '8444461749428370424248824938781546531375899335154063827935233455917409239041'
        custom_prime_length: null
        compiler_args: [ '-F 251', '-C', '--budget', '10000', '-Y']
        script_args:
          dataset: "adult"

$ETL$:
  extract_mpspdz:
    experiments: "*"
    extractors:
      MpSpdzStderrExtractor: {}
      MpSpdzResultExtractor: {}
      IgnoreExtractor: { file_regex: ["^consistency.*\\.log$", "^stdout\\.log$"] }
    transformers: []
    loaders:
      CsvSummaryLoader: {}
  extract_consistency:
    experiments: "*"
    extractors:
      ConsistencyExtractor: { }
      IgnoreExtractor: { file_regex: ["^stdout\\.log$", "^stderr\\.log$", "^result-.*\\.txt$"] }
    transformers: [ ]
    loaders:
      CsvSummaryLoader: {}