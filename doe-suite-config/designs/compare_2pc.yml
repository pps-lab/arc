$SUITE_VARS$:
  sleep_time: 2.0 # TODO [hly] can remove (also from experiment runner) as it does not have any effect
  _python_pre: "export PYTHONPATH={{ exp_code_dir }}/utils && . {{ exp_code_dir }}/.venv/bin/activate"
  _python_path: "[% my_run._python_pre %] && {{ exp_code_dir }}/.venv/bin/python"
  cmd: "[% my_run._python_path %] -m  python_utils.scripts.experiment_runner --player-number <PLAYER-ID> --sleep-time [% my_run.sleep_time %]"


# TODO [hly]: The knn algorithm is still buggy. The l2 distance should be fine but either in the model/data loading or afterward there is a problem

twopc:
  n_repetitions: 1
  host_types:
    compute:
      n: 2
      check_status: True
      init_roles:
        - setup-base
#        - setup-network-delay # adjust latency + bandwidth limit -> doe-suite-config/roles/setup-network-delay/vars/main.yml
        - download-dataset # re-enable for fresh install
      $CMD$:
      - main: "[% my_run.cmd | replace('<PLAYER-ID>', 0) %]"
      - main: "[% my_run.cmd | replace('<PLAYER-ID>', 1) %]"

  base_experiment:
    mpc:
      player_0_hostname: "[% exp_host_lst | json_query('[?host_type==`compute`].private_dns_name') | default(['<UNDEFINED-DNS>'], true) | first %]"
      abs_path_to_code_dir: "{{ exp_code_dir }}"
      player_count: 2
      protocol_setup: "mascot-party"

      compiler_args: ['-F', "64", '-C', '--budget', '10000'] #'100000000'] # budget was '1000000'1million # also look at B 5 -> cannot be used together with R

#      script_name: "audit_sample_knnshapley"
      script_name:
        $FACTOR$: ["audit_sample_knnshapley", "torch_cifar_alex_train" ] # + some inference?
#        $FACTOR$: [ "audit_sample_knnshapley" ] # + some inference?

      #        $FACTOR$: [ "torch_cifar_alex_train" ] # + some inference?
      script_args:
        debug: False  # for a final benchmarking run, can also use `debug: False`, this should then only release no secret info
        emulate: False
        dataset: "cifar_alexnet_3party"
        trunc_pr: False

        # in a network with latency, a high batch size for ml.py is key to a good performance (60k raised insufficient memory error)
        batch_size: 128 #1024 #256 # was 128 before   # 60000 -> insufficient memory
        n_threads: 8 #128 #64 # TODO before was 32 -> see if this helps (128 raises too many files runtime error)

        # inference specific
        n_samples: 10

      stage: # "compile" in run and then in next run "run"
        $FACTOR$: [compile, run] # ensure that this is last factor

$ETL$:
  extract_mpspdz:
    experiments: "*"
    extractors:
      MpSpdzStderrExtractor:  {}
      MpSpdzResultExtractor: {}
      IgnoreExtractor: {}
    transformers: []
    loaders:
      CsvSummaryLoader:
        output_dir: etl_results