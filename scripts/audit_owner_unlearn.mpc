from Compiler.script_utils import output_utils

from Compiler.script_utils.data import data


from Compiler import ml
from Compiler import library

from Compiler.script_utils.audit import owner_unlearn

from Compiler.script_utils import config, timers


class AuditConfig(config.BaseAuditModel):
    learning_rate: float = 0.01
    mod_zscore_threshold: float = 2.5
    n_unlearn_epochs: int = 1

program.options_from_args()
cfg = config.from_program_args(program.args, AuditConfig)

program.use_trunc_pr = cfg.trunc_pr
sfix.round_nearest = cfg.round_nearest
ml.set_n_threads(cfg.n_threads)

library.start_timer(timer_id=timers.TIMER_LOAD_DATA)
input_loader = data.get_input_loader(dataset=cfg.dataset, audit_trigger_idx=cfg.audit_trigger_idx, batch_size=cfg.batch_size, debug=cfg.debug, emulate=cfg.emulate, consistency_check=cfg.consistency_check, sha3_approx_factor=cfg.sha3_approx_factor)
library.stop_timer(timer_id=timers.TIMER_LOAD_DATA)

library.start_timer(timer_id=timers.TIMER_AUDIT)
result = owner_unlearn.audit(input_loader, config=cfg, debug=cfg.debug)


# TODO: should probably think about the output here -> can we define a structured output form that
#       applies to all audit functions
#       maybe could distinguish between "reveal" output vs "debugging output"
# Party Level: [{"party": 3, "score": 0.9}], also []
# Sample Level: [{"party": 3, "sample_id": 1, "score": 0.7}]
# Feature Level: [{"feature_id": "col1", "score": 0.9}]  -> this would be more complex for some feature audit methods

library.stop_timer(timer_id=timers.TIMER_AUDIT)


for k, v in result.items():
    output_utils.output_value_debug(name=k, value=v, repeat=False)
