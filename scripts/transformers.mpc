
import ml
import math

#ml.report_progress = True

# or: 64, 36?
# sfix.set_precision(18, 32)
# sfix.set_precision(32, 62) # needed for GeLu precision

# with higher precision we get wrong values?

from transformers import BertModel, BertForSequenceClassification, BertTokenizer

# Load pre-trained BERT model and tokenizer
model_name = 'prajjwal1/bert-tiny-mnli'  # You can choose other versions of BERT like 'bert-large-uncased'

# Load the tokenizer
tokenizer = BertTokenizer.from_pretrained(model_name)

# Load the BERT model
model = BertForSequenceClassification.from_pretrained(model_name)

# Example of using the tokenizer and model
text = "Hello, how are you?"
encoded_input = tokenizer(text, return_tensors='pt')  # 'pt' refers to PyTorch tensors
output = model(**encoded_input)

program.use_trunc_pr = False
sfix.round_nearest = True

# Output of the model includes hidden states and attention details
print(encoded_input)
print(output)

# get embedding of encoded_input
embedding = model.bert.embeddings(encoded_input['input_ids'], token_type_ids=encoded_input['token_type_ids']).detach()
print("Embed", embedding)

X_test_input_ids = sfix.input_tensor_via(0, encoded_input['input_ids'].numpy())
X_test_token_type_ids = sfix.input_tensor_via(0, encoded_input['token_type_ids'].numpy())
X_test_attention_mask = sfix.input_tensor_via(0, encoded_input['attention_mask'].numpy())

X_test_embed = sfix.input_tensor_via(0, embedding.numpy())
print("X_test_embed.shape", X_test_embed.shape)

layers = ml.layers_from_torch(model, X_test_input_ids.shape, input_via=0, batch_size=1)

print("LAYERS")
print(layers)

program.options_from_args()

batch_size = 64
ml.Layer.back_batch_size = batch_size
N = 1000
n_test = 100
n_features = 28 ** 2
n_inner = 128
activation='relu'
approx=False

optimizer = ml.Optimizer(layers)

ALL_LAYERS = optimizer.layers

print_ln("LINEAR Layer weights before %s: %s", layers[-2], layers[-2].W.reveal_nested())

# optimizer.reset()
# optimizer.print_random_update = True
pred = optimizer.eval(X_test_embed, top=True)

print_ln("LINEAR Layer weights after %s: %s", layers[-2], layers[-2].W.reveal_nested())


if True:
    # we dont test the opening part
    # print_ln('Truth %s', (y_test).reveal_nested())
    print_ln('Prediction %s', (pred).reveal_nested())
    print_ln('Logits %s', (optimizer.layers[-1].X).reveal_nested())
    # print_ln('Difference %s', (Matrix.create_from(pred) - y_test).reveal_nested())
    # print_ln('Secure test loss: %s',
    #          ((sum((pred[:] - y_test[:]) ** 2)) /
    #           (y.shape[1] * len(y_test))).reveal())

# expected output
import torch

expected_layer_outputs = [
    (0, [-0.3623,  0.6323, -6.2308, -0.2590,  0.0424,  0.8070, -0.2308,  1.0920,
          -0.8958,  0.5055, -0.5433,  0.2326,  0.5350,  1.0560,  0.4876, -0.1159,
          0.7600,  0.6899,  0.0085,  0.6060, -0.3182, -0.2685,  2.1104,  0.7764,
          0.3459, -1.0142, -0.3353,  0.0341, -0.0593, -0.6760, -0.4526, -0.0958,
          -2.9853, -0.7197,  0.1703,  0.1358,  0.7969,  0.3830, -0.3987, -0.9182,
          0.7203,  0.0928,  0.2243, -0.9618, -0.1236, -0.6443, -0.4000, -0.1625,
          -0.5498,  0.1734, -0.7358,  0.7184,  0.1816, -0.2514,  0.7301,  0.2117,
          0.2490,  0.1735, -0.0938,  0.3605,  0.3827,  0.0786, -0.0811, -0.1628,
          0.1419,  0.2227,  0.0771, -0.3171,  0.3987,  0.3395, -0.4622, -0.5655,
          0.8045, -1.0183,  0.3114,  0.3477,  0.2678,  0.9881,  1.6028,  0.1114,
          1.2723, -0.2098,  0.1590,  0.6822, -0.5610, -0.6971,  0.3647,  0.8419,
          0.9248, -0.9224,  0.6209,  0.0160,  0.0120,  0.1568,  0.9768,  0.4712,
          -1.0321, -0.1407,  0.3784,  0.6088,  0.4269,  0.5820,  1.4699,  0.8049,
          0.2594, -0.1552,  0.7604, -0.7375,  0.3782,  0.0927, -0.1062,  0.5293,
          0.0078, -0.5184, -0.3641,  0.7294,  0.6970, -0.4448,  1.2082,  0.6928,
          0.0965, -0.3048,  0.3609, -0.4793, -0.4731, -0.2811, -0.1318,  0.2686]),
    (1, [-0.4994,  0.0646, -2.1017, -1.6386, -1.7391, -1.6115,  0.1350,  1.1529,
         -0.3701, -0.9501,  0.6732,  0.7286,  1.3009, -0.3318,  1.4034, -0.8896,
         -0.2596, -0.4610, -1.3565, -0.0348, -1.2974,  0.4436,  1.4571,  0.1530,
         0.5041, -2.1923,  0.5743, -0.1579, -1.1061, -2.0935, -0.0533, -2.2221,
         -1.9281, -0.7219, -0.3273, -0.1921,  0.8991,  1.2826, -2.7867, -0.1391,
         1.1970, -0.8486,  0.4043, -1.1644,  0.7544, -1.7207, -0.9795,  0.2733,
         1.4036, -1.8313, -0.8759,  1.6346,  0.1232,  0.7674,  1.5569, -1.6812,
         0.6873, -0.9044, -1.4062,  0.2777,  0.6882, -0.4774, -0.6360, -0.2981,
         0.0109,  0.4105, -1.5826, -0.2450,  2.6447,  0.0905, -0.7159, -0.5499,
         1.4409,  0.9610, -0.4100,  0.0520, -1.4193,  1.3826,  1.4320,  1.3571,
         0.6764,  1.6044,  0.9054,  0.7657, -0.6801, -0.5296, -0.9206,  0.4697,
         0.4317,  0.2260,  1.1927,  0.2785, -0.3482,  2.0948,  0.5160,  0.8951,
         0.8179,  0.4663,  1.1232,  1.2681,  0.1654,  1.4321,  0.7440, -0.2537,
         1.0815, -1.0828,  0.2663, -1.2794, -0.7743, -0.0144,  1.2978,  0.1031,
         0.4545, -1.1103, -0.6056, -0.0369,  0.0220, -0.0951,  1.7135, -0.2620,
         -1.2409,  0.4394, -0.3739, -2.2301, -2.0754,  1.0332,  0.2485, -1.5314])
]

for (layer_id, pt) in expected_layer_outputs:
    runtime_output = optimizer.layers[layer_id].Y.reveal_nested()[0][0]
    print_ln("layer %s, %s", layer_id, optimizer.layers[layer_id])
    # print("runtime_output", runtime_output, len(runtime_output))
    # for e in pt:
    #     print_str("%s == %s, ", e, runtime_output.pop(0))
    diff = cfix(0)
    for i in range(len(pt)):
        diff += (pt[i] - runtime_output[i]) ** 2
    print_ln("diff %s", diff)

    print_ln(" ")

print_ln(" ")
print_ln("Pool layer")
print_ln(" ")

expected_tensor_outputs = [-0.9929,  0.5423, -0.9741, -0.8603, -0.9989, -0.2985, -0.9845, -0.8301,
                          -0.2662, -0.5004, -0.9250, -0.2557, -0.4657,  1.0000, -0.9945, -0.6968,
                          -0.5956,  0.4347, -0.9999,  0.8759,  0.9294,  0.1812,  0.9963, -0.0321,
                          -0.9525, -0.0912, -0.9999, -0.9900,  0.9931, -0.1796, -0.3511,  0.1675,
                          -0.9309,  0.4465, -0.9815,  0.9989,  0.4739, -0.4157,  0.0345, -0.9752,
                          0.9768,  0.9987, -0.9818,  0.7943, -1.0000, -0.0858, -0.9943,  0.9989,
                          0.9363,  0.9353, -0.4612,  0.0539, -0.0791, -0.9860,  0.9774,  1.0000,
                          -1.0000,  0.7750, -0.2920, -0.2795,  0.1772, -0.7358,  0.1101, -0.1506,
                          -0.8505, -1.0000, -0.9761, -0.7718,  0.9992,  0.9527,  0.9995,  0.5004,
                          -0.9912,  0.3132,  0.7610, -0.9814, -0.1393,  0.6704, -0.2549,  0.4231,
                          0.8687, -0.0066,  0.3030, -0.9987,  0.9771, -0.9971,  0.8584,  0.9865,
                          0.6218,  0.8037, -0.9999, -0.9598, -0.5499, -0.5201, -0.0601, -0.8656,
                          0.7757,  0.9774, -0.9832, -0.8132, -0.3835,  0.9099, -0.9795, -0.9452,
                          -0.9906, -0.2960, -0.9993, -0.9133,  0.3014,  0.4302,  0.9824, -0.8322,
                          -0.4418,  0.9990, -1.0000, -0.1486,  0.3582,  0.6761,  0.3975,  0.1415,
                          0.3896, -0.9999, -0.2399,  0.9710, -0.9754, -0.3524,  0.8846,  0.6676]

# runtime_output = optimizer.layers[-2].X.reveal_nested()[0][0]
runtime_output = optimizer.layers[-4].Y.reveal_nested()[0]
print("layer -4", optimizer.layers[-4])
# print("runtime_output", runtime_output, len(runtime_output))
diff = cfix(0)
for i in range(len(expected_tensor_outputs)):
    diff += (expected_tensor_outputs[i] - runtime_output[i]) ** 2
print_ln("diff %s", diff)
# for e in expected_tensor_outputs:
#     # print_str("%s == %s, ", e, runtime_output.pop(0))
#     print_str("%s, ", runtime_output.pop(0))

last_layer_weights = optimizer.layers[-2].W.reveal_nested()
last_layer_bias = optimizer.layers[-2].b.reveal_nested()

# sum
sum_last_layer_weights = cfix(0)
for i in range(len(last_layer_weights)):
    for j in range(len(last_layer_weights[i])):
        sum_last_layer_weights += last_layer_weights[i][j]
print_ln("sum_last_layer_weights %s", sum_last_layer_weights)

print_ln("last_layer_weights %s %s ", last_layer_weights, last_layer_bias)

# library.stop_timer(timer_id=114)

