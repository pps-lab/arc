import math

# Given is a trained model and possible malicious data

# Do some preparations

program.options_from_args()

n_thread_num = int(program.args[1])

# training_sample_size = sint.input_from(0)

training_samples = MultiArray([60000, 28, 28], sfix)
training_labels = MultiArray([60000, 10], sint)

test_samples = MultiArray([10000, 28, 28], sfix)
test_labels = MultiArray([10000,10], sint)

n_samples = 8
prediction_samples = sfix.Tensor([n_samples, 28, 28])
prediction_labels = sint.Tensor([n_samples, 10])

# Load the actual data
training_labels.input_from(0)
training_samples.input_from(0)
test_labels.input_from(0)
test_samples.input_from(0)
prediction_labels.input_from(0)
prediction_samples.input_from(0)

# Prepare the model
from Compiler import ml
tf = ml

tf.set_n_threads(n_thread_num)


layers = [
    tf.keras.layers.Conv2D(20,5,1,'valid', activation='relu'),
    tf.keras.layers.MaxPooling2D(2),
    tf.keras.layers.Conv2D(50,5,1,'valid', activation='relu'),
    tf.keras.layers.MaxPooling2D(2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(500, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
]

model = tf.keras.models.Sequential(layers)
optim = tf.keras.optimizers.SGD()

model.compile(optimizer=optim)
model.build(test_samples.sizes)

start = 0
variable_holders = []
for i,var in enumerate(model.trainable_variables):
    print(f"trainable_variable {i}: {var}")
    print_ln("Loading trainable_variable %s",i)
    var.input_from(0)
    if isinstance(var, MultiArray):
        copy_container = MultiArray(var.sizes,var.value_type)
        copy_container.assign(var)
        variable_holders.append(copy_container)
    elif isinstance(var, Array):
        copy_container = Array(var.length, var.value_type)
        copy_container.assign(var)
        variable_holders.append(copy_container)


print(variable_holders)

# Now, since we hold a copy of the trained variables, we can start with doing the actual training
# We first assume a split 5 datasets



# Start the training loop
models = []
model_losses = []
unlearn_labels = MultiArray([20_000,10],sfix)
unlearn_labels.assign_all(sfix(1/10))

print_ln("Start Unlearning")

def compute_loss(model, training_data, training_labels, batch_size, array_size):
    # We get the model
    print_ln("Starting compute loss")
    self = model.opt
    the_index = MemValue(cint(0))
    loss_array = Array(array_size,sfix)
    def f(start, batch_size, batch):
        print_ln("Running computation for batch %s", start)
        batch.assign_vector(regint.inc(batch_size,start))
        print_ln("Assigning batch")
        self.forward(batch=batch)
        print_ln("Ran self.forward")
        index_start = start // batch_size
        curr_loss = self.layers[-1].l * batch_size
        print_ln("Current_loss %s", curr_loss.reveal())
        loss_array.assign(curr_loss,the_index.read()) 
        print_ln("Added losses")
        the_index.iadd(cint(1))
        print_ln("the_index: %s", the_index.read())
    print_ln("Starting run in batches")
    self.run_in_batches(f, training_data, batch_size,truth=training_labels)
    loss = cfix(0)
    @map_sum_opt(1,array_size, [cfix])
    def loss_sum(i):
        return loss_array.get(i).reveal() 
    loss = loss_sum()
    return loss


def modified_run(self2,batch_size=None, stop_on_loss=0,loss_carrier=None):
    self = self2
    if self.n_epochs == 0:
        return 
    if batch_size is not None:
        N = batch_size
    else:
        N = self.layers[0].N
    
    i = self.i_epoch
    n_iterations = MemValue(0)
    self.n_correct = MemValue(0)
    @for_range(self.n_epochs)
    def _(_):
        if self.X_by_label is None:
            self.X_by_label = [[None] * self.layers[0].N]
        assert len(self.X_by_label) in (1,2)
        assert N % len(self.X_by_label) == 0
        n = N // len(self.X_by_label)
        n_per_epoch = int(math.ceil(1. * max(len(X) for X in self.X_by_label) / n))
        print("%d runs per epoch" % n_per_epoch)
        indices_by_label = []
        for label, X in enumerate(self.X_by_label):
            indices = regint.Array(n * n_per_epoch)
            indices_by_label.append(indices)
            indices.assign(regint.inc(len(X)))
            missing = len(indices) - len(X)
            if missing:
                indices.assign_vector(
                    regint.get_random(int(math.log2(len(X))),size=missing),
                    base=len(X)
                )
            if self.always_shuffle or n_per_epoch > 1:
                indices.shuffle()
        loss_sum = MemValue(sfix(0))
        self.n_correct.write(0)
        @for_range(n_per_epoch)
        def _(j):
            n_iterations.iadd(1)
            batch = regint.Array(N)
            for label, X in enumerate(self.X_by_label):
                indices = indices_by_label[label]
                batch.assign(
                    indices.get_vector(j * n, n) + 
                    regint(label * len(self.X_by_label[0]), size=n),
                    label * n
                )    
            self.forward(batch=batch, training=True)
            self.backward(batch=batch)
            if self.time_layers:
                start_timer(1000)
            self.update(i, batch=batch)
            if self.time_layers:
                stop_timer(1000)
            loss_sum.iadd(self.layers[-1].l)
            if self.print_loss_reduction:
                before = self.layers[-1].average_loss(N)
                self.forward(batch=batch)
                after = self.layers[-1].average_loss(N)
                print_ln(
                    'loss reduction in batch %s: %s (%s - %s)',
                    j,
                    before - after, before, after
                )
            elif self.print_losses:
                print_str('\rloss in batch %s: %s/%s', j,
                             self.layers[-1].average_loss(N),
                             loss_sum.reveal() / (j + 1))
            if self.reveal_correctness:
                part_truth = self.layers[-1].Y.same_shape()
                part_truth.assign_vector(
                        self.layers[-1].Y.get_slice_vector(batch))
                self.n_correct.iadd(
                        self.layers[-1].reveal_correctness(batch_size, part_truth))
            if stop_on_loss:
                loss = self.layers[-1].average_loss(N)
                res = (loss < stop_on_loss) * (loss >= -1)
                self.stopped_on_loss.write(1-res)
                return res
            if self.print_losses:
                print_ln()
            if self.report_loss and self.layers[-1].compute_loss and self.layers[-1].approx != 5:
                print_ln('loss in epoch %s: %s', i,
                         (loss_sum.reveal() * cfix(1 / n_per_epoch)))
                if loss_carrier is not None:
                    loss_carrier.write(loss_sum.reveal() * cfix(1 / n_per_epoch))
            else:
                print_ln("Done with epoch %s", i)
            time()
            i.iadd(1)
            res = True
            if self.tol > 0:
                res *= (1 - (loss_sum >= 0) * \
                        (loss_sum < self.tol * n_per_epoch)).reveal()
            self.stopped_on_low_loss.write(1 - res)
            return res





# Split the set of programs into 3 parts
for i in range(3):
    print_ln("Unlearning model %s", i)
    current_model = tf.keras.models.Sequential(layers)
    current_optim = tf.keras.optimizers.SGD()
    current_model.compile(optimizer=current_optim)

    # Build unlearn dataset 
    # We only need to modify the training_labels for the unleared dataset
    unlearn_start = i * 20_000 # as we have 60_000 / 3 = 20_000
    unlearn_size = 20_000
    batch_size = 128
    modified_training_labels = MultiArray([60_000, 10], sfix)
    modified_training_labels.assign(training_labels)
    # Now, we set the portion of the training labels that we want to unlearn
    modified_training_labels.get_part(unlearn_start,unlearn_size).assign(unlearn_labels)
    current_model.build(test_samples.sizes, batch_size)
    # Load the checkpoint for the model
    print_ln("Loading checkpoint for model %s", i)
    for var,check_point in zip(current_model.trainable_variables,variable_holders):
        var.assign(check_point)
    
    # Now, we set input and output
    current_model.opt.layers[0].X.address = training_samples.address
    current_model.opt.layers[-1].Y.address = modified_training_labels.address
    current_model.opt.n_epochs = 1
    current_model.opt.i_epoch.write(0)
    current_model.opt.stopped_on_loss.write(0)

    print_ln("Start unlearning")
    my_loss_carrier = MemValue(cfix(0))
    #current_model.opt.run(batch_size=128,stop_on_loss=0,loss_carrier=)
    modified_run(current_model.opt, batch_size=128, stop_on_loss=0,)
    models.append(current_model)
    print_ln("Compute losses for model %s", i)
    # _,current_model_loss = current_model.opt.reveal_correctness(training_samples, modified_training_labels,128)
    # model_losses.append(current_model_loss)

    # Only compute loss for model 0
    if i == 0:
        model_loss = loss_carrier.read()
        print_ln("Model loss for Model 0: %s", model_loss)




        



# Now, we calculate the MAD score
mad_scores = sint.Array(5)
# We now print out the loss scores:
#for i in range(3):
#    print_ln("Model %s: Loss %s", i, model_losses[i])







    
    





