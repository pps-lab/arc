from Compiler.script_utils import output_utils

from Compiler.script_utils.data import data
from Compiler.script_utils.data import AbstractInputLoader


from Compiler import ml
from Compiler import library

from Compiler.script_utils.audit import shap

from Compiler.script_utils import config, timers, input_consistency




class TrainingConfig(config.BaseAuditModel):
    n_epochs: int = 1 # -1 = all

    n_s_bert: int = 500

program.options_from_args()
cfg = config.from_program_args(program.args, TrainingConfig)

MultiArray.disable_index_checks()
Array.check_indices = False

if not cfg.emulate:
    pass
    # program.use_trunc_pr = cfg.trunc_pr
    # program.use_edabits = True
    # program.use_split(4)

# program.use_edabit(False)
# program.use_dabit = False
# program.use_split(3)

# program.set_bit_length(32)

program.use_trunc_pr = cfg.trunc_pr

sfix.round_nearest = cfg.round_nearest

ml.set_n_threads(cfg.n_threads)
ml.Layer.back_batch_size = cfg.batch_size
ml.Layer.debug_bert_output = cfg.debug

library.start_timer(timer_id=timers.TIMER_LOAD_DATA)
input_shape_size = cfg.batch_size * cfg.n_batches if cfg.n_batches > 0 else None
input_loader: AbstractInputLoader = data.get_input_loader(dataset=cfg.dataset, audit_trigger_idx=cfg.audit_trigger_idx,
                                                                    batch_size=cfg.batch_size, debug=cfg.debug, emulate=cfg.emulate, consistency_check=cfg.consistency_check, sha3_approx_factor=cfg.sha3_approx_factor,
                                                          load_model_weights=False, input_shape_size=input_shape_size, n_train_samples_bert=cfg.n_s_bert
                                                          )
library.stop_timer(timer_id=timers.TIMER_LOAD_DATA)


library.start_timer(timer_id=timers.TIMER_TRAINING)

# eval here
train_samples, train_labels = input_loader.train_dataset() # train dataset in case we dont have test dataset

print(train_samples.sizes, "TRAIN")
model = input_loader.model()
model.summary()

model.reset()

if cfg.n_batches > 0:
    train_samples = train_samples.get_part(0, cfg.n_batches * cfg.batch_size)
    print("Train_samples", train_samples, train_samples)
    train_labels = train_labels.get_part(0, cfg.n_batches * cfg.batch_size)
    print("Running on", len(train_samples), "samples")
    model.layers[-1].Y = train_labels
    model.layers[0].X = train_samples
    print(model.layers[0])
    # model.layers[0].N = cfg.n_batches * cfg.batch_size
else:
    model.layers[-1].Y.address = train_labels.address
    model.layers[0].X.address = train_samples.address

print(model.layers)

# n_test = 10000
# n_features = 784
# Y = sint.Array(n_test)
# X = sfix.Matrix(n_test, n_features)
# Y.assign_all(0)
# X.assign_all(0)
# model.run(batch_size=cfg.batch_size)
# model.run_by_args()

# print(program.args)
# program.args.append('print_losses')

# disable early stopping
program.args.append('no_loss')

# program.use_trunc_pr = True

# optim = ml.Optimizer.from_args(program, model.layers)
# model.set_learning_rate(10.0)
# model.print_losses = True

# okay... we need this apparently
# this determines whether we compute the loss on the forward pass
model.layers[-1].compute_loss = False
# model.print_losses = True

model.set_learning_rate(0.001)

# model.time_layers = True

if cfg.n_epochs == 0:
    print("Skipping training because n_epochs=0")
else:
    model.run(batch_size=cfg.batch_size)

# out = Array(2977, sfix)
# out.assign_all(-2)
# out[0] = 2
# out[1] = 2
# out[3] = 3
# import numpy as np
# for i in range(2977):
#     out[i] = np.random.randint(-16000, 16000)
# sfix.write_to_file(out)
# input_consistency.output(out, None, None)


# model.fit(
#     train_samples,
#     train_labels,
#     epochs=int(cfg.n_epochs),
#     batch_size=128,
#     program=program,
#     print_accuracy=True
# )
# prediction_results = model.eval(inf_samples, batch_size=min(cfg.batch_size, cfg.n_samples))
# n_correct, avg_loss = model.reveal_correctness(data=inf_samples, truth=inf_labels, batch_size=input_loader.batch_size(), running=True)
# print_ln("  n_correct=%s  n_samples=%s  avg_loss=%s", n_correct, len(inf_samples), avg_loss)

library.stop_timer(timer_id=timers.TIMER_TRAINING)

library.start_timer(timer_id=timers.TIMER_OUTPUT_COMMIT)

model_layers = AbstractInputLoader._extract_model_weights(model)

output_object = input_consistency.InputObject(model=model_layers)
input_consistency.output(output_object, cfg.consistency_check, cfg.n_threads, cfg.sha3_approx_factor, cfg.cerebro_output_approx_factor)

library.stop_timer(timer_id=timers.TIMER_OUTPUT_COMMIT)


# if cfg.debug:
#     print_ln(prediction_results.reveal(), inf_labels.reveal())
