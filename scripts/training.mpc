from Compiler.script_utils import output_utils

from Compiler.script_utils.data import data
from Compiler.script_utils.data import AbstractInputLoader


from Compiler import ml
from Compiler import library

from Compiler.script_utils.audit import shap

from Compiler.script_utils import config, timers, input_consistency




class TrainingConfig(config.BaseAuditModel):
    n_epochs: int = 1 # -1 = all

program.options_from_args()
cfg = config.from_program_args(program.args, TrainingConfig)

MultiArray.disable_index_checks()
Array.check_indices = False

if not cfg.emulate:
    pass
    # program.use_trunc_pr = cfg.trunc_pr
    # program.use_edabits = True
    # program.use_split(4)

# program.use_edabit(False)
# program.use_dabit = False
# program.use_split(3)

# program.set_bit_length(32)

program.use_trunc_pr = cfg.trunc_pr

ml.set_n_threads(cfg.n_threads)
ml.Layer.back_batch_size = cfg.batch_size

library.start_timer(timer_id=timers.TIMER_LOAD_DATA)
input_loader: AbstractInputLoader = data.get_input_loader(dataset=cfg.dataset, audit_trigger_idx=cfg.audit_trigger_idx,
                                                                    batch_size=cfg.batch_size, debug=cfg.debug, emulate=cfg.emulate, consistency_check=cfg.consistency_check,
                                                          load_model_weights=False
                                                          )
library.stop_timer(timer_id=timers.TIMER_LOAD_DATA)


library.start_timer(timer_id=timers.TIMER_TRAINING)

# eval here
train_samples, train_labels = input_loader.train_dataset() # train dataset in case we dont have test dataset

print(train_samples.sizes, "TRAIN")
model = input_loader.model()
model.summary()

model.reset()

model.layers[-1].Y.address = train_labels.address
model.layers[0].X.address = train_samples.address

print(model.layers)

# n_test = 10000
# n_features = 784
# Y = sint.Array(n_test)
# X = sfix.Matrix(n_test, n_features)
# Y.assign_all(0)
# X.assign_all(0)
# model.run(batch_size=cfg.batch_size)
# model.run_by_args()

# print(program.args)
# program.args.append('print_losses')

# disable early stopping
program.args.append('no_loss')

# program.use_trunc_pr = True

# optim = ml.Optimizer.from_args(program, model.layers)
# optim.print_losses = True
# model.set_learning_rate(10.0)
# model.print_losses = True

# okay... we need this apparently
# this determines whether we compute the loss on the forward pass
model.layers[-1].compute_loss = False
# model.time_layers = True

# model.run_by_args(program, cfg.n_epochs, cfg.batch_size, False, False, reset=False)
# model.run(batch_size=cfg.batch_size)

# model.fit(
#     train_samples,
#     train_labels,
#     epochs=int(cfg.n_epochs),
#     batch_size=128,
#     program=program,
#     print_accuracy=True
# )
# prediction_results = model.eval(inf_samples, batch_size=min(cfg.batch_size, cfg.n_samples))
# n_correct, avg_loss = model.reveal_correctness(data=inf_samples, truth=inf_labels, batch_size=input_loader.batch_size(), running=True)
# print_ln("  n_correct=%s  n_samples=%s  avg_loss=%s", n_correct, len(inf_samples), avg_loss)

library.stop_timer(timer_id=timers.TIMER_TRAINING)

library.start_timer(timer_id=timers.TIMER_OUTPUT_COMMIT)
input_consistency.output(model, None, None)
library.stop_timer(timer_id=timers.TIMER_OUTPUT_COMMIT)


# if cfg.debug:
#     print_ln(prediction_results.reveal(), inf_labels.reveal())
