
import Compiler.ml as ml
import torch
import torch.nn as nn
import numpy as np
from transformers import BertForSequenceClassification
from transformers.models.bert.modeling_bert import BertAttention

# Set random seed for reproducibility
np.random.seed(0)
torch.manual_seed(0)

cfix.set_precision(f=32, k=62)
sfix.set_precision(f=32, k=62)

# Parameters
N = 1
seq_len = 5
hidden_size = 128
num_attention_heads = 2

wv = torch.randn(hidden_size, hidden_size)
wq = torch.randn(hidden_size, hidden_size)
wk = torch.randn(hidden_size, hidden_size)
output_w = torch.randn(hidden_size, hidden_size)
layernorm_w = torch.randn(hidden_size)

secret_wv = sfix.input_tensor_via(0, wv)


# Initialize both layers
attention = ml.MultiHeadAttention(N, seq_len, hidden_size, num_attention_heads, dropout=0.25, rsqrt_approx=False)
attention.wq.W = sfix.input_tensor_via(0, wq)
attention.wk.W = sfix.input_tensor_via(0, wk)
attention.wv.W.address = secret_wv.address
attention.wq.b.assign_all(sfix(0))
attention.wk.b.assign_all(sfix(0))
attention.wv.b.assign_all(sfix(0))
attention.output.dense.W = sfix.input_tensor_via(0, output_w)
attention.output.dense.b.assign_all(sfix(0))
attention.output.layer_norm.weights = sfix.input_tensor_via(0, layernorm_w)
attention.output.layer_norm.bias.assign_all(sfix(0))


model_name = 'M-FAC/bert-tiny-finetuned-qnli'
model = BertForSequenceClassification.from_pretrained(model_name)
print(model.config)

pt_attn = BertAttention(model.config)
pt_attn.self.query.weight.data = wq.T
pt_attn.self.query.bias.data = torch.zeros(hidden_size)
pt_attn.self.key.weight.data = wk.T
pt_attn.self.key.bias.data = torch.zeros(hidden_size)
pt_attn.self.value.weight.data = wv.T
pt_attn.self.value.bias.data = torch.zeros(hidden_size)
pt_attn.output.dense.weight.data = output_w.T
pt_attn.output.dense.bias.data = torch.zeros(hidden_size)
pt_attn.output.LayerNorm.weight.data = layernorm_w
pt_attn.output.LayerNorm.bias.data = torch.zeros(hidden_size)
pt_attn.eval()

# Input tensor
pre_tensor = torch.randn(N, seq_len, hidden_size)
pre_tensor /= 100
input_tensor = torch.tensor(pre_tensor, requires_grad=True)
# input_tensor = torch.zeros(N, seq_len, hidden_size, requires_grad=True)
hidden_state = torch.randn(N, seq_len, hidden_size, requires_grad=True)
secret_hidden_state = sfix.input_tensor_via(0, hidden_state.detach())
secret_hidden_state.assign_all(0)

# Convert input for FlexDense using sfix
flex_input = sfix.input_tensor_via(0, input_tensor.detach())

attention.X.address = flex_input.address

inc_batch = regint.Array(N)
inc_batch.assign(regint.inc(N))


activation_map = {}
def get_activation(name):
    def hook(model, input, output):
        print(f"{name} Activation Input", input[0].shape, "shape", output[0].shape)
        # activation_list.append((name, output[0].detach()))
        activation_map[name] = output[0].detach()
    return hook

pt_attn.self.dropout.register_full_backward_hook(get_activation('Self Dropout'))
# pt_attn.self.value.register_full_backward_hook(get_activation('Self Value'))
pt_attn.self.key.register_full_backward_hook(get_activation('wk.nabla_Y'))
pt_attn.self.query.register_full_backward_hook(get_activation('wq.nabla_Y'))
# pt_attn.self.value.register_full_backward_hook(get_activation('Self Output'))
# pt_attn.self.register_full_backward_hook(get_activation('Self'))


# Get outputs
attention.forward(inc_batch, flex_input, training=True)
flex_output = attention.Y
torch_output = pt_attn(input_tensor)

print_ln("Wv input 0 and weight 0 %s %s", attention.wv.W.reveal_nested()[0][0][0], flex_input.reveal_nested()[0][0][0])
# Compare outputs
print_ln("Attention Output: %s", flex_output.reveal_nested()[0][0][:16])
print_ln("Torch Attention Output: %s", torch_output[0][0][0][:16])

# wv_output = pt_attn.self(input_tensor)
# wv_dense_out = pt_attn.output.dense(wv_output[0])
# print_ln("Torch Self Output: %s", wv_output[0][0][0][:16])
# print_ln("Torch Dense Output: %s", wv_dense_out[0][0][:16])
# print_ln("Torch Dense Output Plus: %s", wv_dense_out[0][0][:16] + input_tensor[0][0][:16])
#
# ln_input = wv_dense_out + input_tensor
# ln_mean = ln_input.mean(dim=-1, keepdim=True)
# ln_var = ln_input.std(dim=-1, keepdim=True, unbiased=False)
# ln_output = layernorm_w * (ln_input - ln_mean) / ln_var
# print_ln("Torch layernorm mean %s", ln_mean)
# print_ln("Torch layernorm var %s", ln_var)
# print_ln("Torch layernorm %s", ln_output[0][0][:16])

# print_ln("Diff %s", flex_output.reveal_nested()[0][0][:16] - torch_output[0][0][0][:16].tolist())

# Backward
grad_y = torch.randn(N, seq_len, hidden_size)

attention.nabla_Y = sfix.input_tensor_via(0, grad_y)
attention.nabla_X.alloc()
attention.backward(True, inc_batch)




torch_output[0].backward(grad_y)

print("activation map", activation_map)

print_ln("Attention backward gradient: %s", attention.nabla_X.reveal_nested()[0][0][:16])
print_ln("Torch backward gradient: %s", input_tensor.grad[0][0][:16])



