
import ml
import math

# evaluates the processing of a single bert sequence

# with higher precision we get wrong values?

from transformers import BertModel, BertTokenizer

# Load pre-trained BERT model and tokenizer
model_name = 'bert-base'  # You can choose other versions of BERT like 'bert-large-uncased'

# Load the tokenizer
tokenizer = BertTokenizer.from_pretrained(model_name)

# Load the BERT model
model = BertModel.from_pretrained(model_name)

# Example of using the tokenizer and model
text = "Hello, how are you? What is the capital of the country of Sweden?"
encoded_input = tokenizer(text, return_tensors='pt')  # 'pt' refers to PyTorch tensors
output = model(**encoded_input)

# Output of the model includes hidden states and attention details
print(encoded_input)
print(output)

# get embedding of encoded_input
embedding = model.bert.embeddings(encoded_input['input_ids'], token_type_ids=encoded_input['token_type_ids']).detach()
print("Embed", embedding)

X_test_embed = sfix.input_tensor_via(0, embedding.numpy())
print("X_test_embed.shape", X_test_embed.shape)

layers = ml.layers_from_torch(model, X_test_embed.shape, input_via=0, batch_size=1)

print("LAYERS")
print(layers)

program.options_from_args()

batch_size = 1
ml.Layer.back_batch_size = batch_size

optimizer = ml.Optimizer(layers)
# optimizer.reset()
# optimizer.print_random_update = True
pred = optimizer.eval(X_test_embed)


if True:
    # we dont test the opening part
    # print_ln('Truth %s', (y_test).reveal_nested())
    print_ln('Prediction %s', (pred).reveal_nested())
    # print_ln('Difference %s', (Matrix.create_from(pred) - y_test).reveal_nested())
    # print_ln('Secure test loss: %s',
    #          ((sum((pred[:] - y_test[:]) ** 2)) /
    #           (y.shape[1] * len(y_test))).reveal())

# library.stop_timer(timer_id=114)

