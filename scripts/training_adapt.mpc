from Compiler.script_utils import output_utils

from Compiler.script_utils.data import data
from Compiler.script_utils.data import AbstractInputLoader


from Compiler import ml
from Compiler import library

from Compiler.script_utils.audit import shap

from Compiler.script_utils import config, timers




class TrainingConfig(config.BaseAuditModel):
    n_epochs: int = 1 # -1 = all

program.options_from_args()
cfg = config.from_program_args(program.args, TrainingConfig)

MultiArray.disable_index_checks()
Array.check_indices = False

if not cfg.emulate:
    pass
    # program.use_trunc_pr = cfg.trunc_pr
    # program.use_edabits = True
    # program.use_split(4)

cfg.n_threads = 36
# program.use_edabit(False)
# program.use_dabit = False
# program.use_split(3)

# program.set_bit_length(32)

ml.set_n_threads(cfg.n_threads)
ml.Layer.back_batch_size = cfg.batch_size

library.start_timer(timer_id=timers.TIMER_LOAD_DATA)
input_loader: AbstractInputLoader = data.get_input_loader(dataset=cfg.dataset, audit_trigger_idx=cfg.audit_trigger_idx,
                                                                    batch_size=cfg.batch_size, debug=cfg.debug, emulate=cfg.emulate, consistency_check=cfg.consistency_check,
                                                          )
library.stop_timer(timer_id=timers.TIMER_LOAD_DATA)


library.start_timer(timer_id=timers.TIMER_TRAINING)

# eval here
train_samples, train_labels = input_loader.train_dataset() # train dataset in case we dont have test dataset


ml.Layer.back_batch_size = cfg.batch_size
n_examples = 60000
N = n_examples
n_test = 10000
n_features = 28 ** 2
layers = [
    ml.FixConv2d([n_examples, 28, 28, 1], (20, 5, 5, 1), (20,), [N, 24, 24, 20], (1, 1), 'VALID'),
    ml.MaxPool([N, 24, 24, 20]),
    ml.Relu([N, 12, 12, 20]),
    ml.FixConv2d([N, 12, 12, 20], (50, 5, 5, 20), (50,), [N, 8, 8, 50], (1, 1), 'VALID'),
    ml.MaxPool([N, 8, 8, 50]),
    ml.Relu([N, 4, 4, 50]),
    ml.Dense(N, 800, 500),
    ml.Relu([N, 500]),
    ml.Dense(N, 500, 10),
]

layers += [ml.MultiOutput.from_args(program, n_examples, 10)]

print(layers)

# program.use_split(3)
program.use_trunc_pr = True

Y = sint.Matrix(n_test, 10)
X = sfix.Matrix(n_test, n_features)

if not ('no_acc' in program.args and 'no_loss' in program.args):
    layers[-1].Y.assign_all(0)
    layers[0].X.assign_all(0)
    Y.assign_all(0)
    X.assign_all(0)

optim = ml.Optimizer.from_args(program, layers)
optim.summary()
optim.run_by_args(program, cfg.n_epochs, cfg.batch_size, False, False)


# disable early stopping
# program.args.append('no_loss')



# model.fit(
#     train_samples,
#     train_labels,
#     epochs=int(cfg.n_epochs),
#     batch_size=128,
#     program=program,
#     print_accuracy=True
# )
# prediction_results = model.eval(inf_samples, batch_size=min(cfg.batch_size, cfg.n_samples))
# n_correct, avg_loss = model.reveal_correctness(data=inf_samples, truth=inf_labels, batch_size=input_loader.batch_size(), running=True)
# print_ln("  n_correct=%s  n_samples=%s  avg_loss=%s", n_correct, len(inf_samples), avg_loss)

library.stop_timer(timer_id=timers.TIMER_TRAINING)

# if cfg.debug:
#     print_ln(prediction_results.reveal(), inf_labels.reveal())
