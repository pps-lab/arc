from Compiler import ml
from Compiler import mpc_math

tf = ml

import math as pymath

true_labels = [
    [0,0,1],
    [1,0,0],
    [0,1,0],
    [1,0,0],
    [1,0,0]
]

prediction_weights = [
    [5,1,1],
    [5,1,1],
    [1,1,5],
    [2,3,2],
    [6,2,2]
]

def softmax_func(x):
    vector_size = len(x)
    exp_vector = [pymath.exp(v) for v in x]
    exp_sum = sum(exp_vector)
    result_vector = [v /exp_sum for v in exp_vector]
    return result_vector

predictions = [softmax_func(x) for x in prediction_weights]


predictions_matrix = MultiArray([len(predictions),len(predictions[0])], sfix)

for i,feature_vector in enumerate(predictions):
    for j,value in enumerate(feature_vector):
        predictions_matrix[i][j] = sfix(value)

true_prediction_matrix = MultiArray([len(true_labels), len(true_labels[0])],sfix)

for i,feature_vector in enumerate(true_labels):
    for j, value in enumerate(feature_vector):
        true_prediction_matrix[i][j] = sfix(value)

## Here starts the MPC part

def compute_cross_entropy(predicted_labels, true_labels):
    sizes = predicted_labels.sizes
    n_features = sizes[1]
    n_rows = sizes[0]
    losses = Array(n_rows,predicted_labels.value_type)
    @for_range(start=0,stop=n_rows,step=1)
    def _(i):
        loss_sum = MemValue(sfix(0))
        @for_range(start=0,stop=n_features,step=1)
        def _(j):
            result_loss = -true_labels[i][j] * tf.log_e(predicted_labels[i][j])
            loss_sum.write(loss_sum.read() + result_loss)
        losses[i] = loss_sum.read()
    return losses


losses = compute_cross_entropy(predictions_matrix, true_prediction_matrix)
print_ln("Lossees %s", losses.reveal_list())




