import itertools

import ml

from datasets import load_dataset
from transformers import BertModel, BertForSequenceClassification, BertTokenizer
import torch

# Load pre-trained BERT model and tokenizer
# model_name = 'prajjwal1/bert-tiny-mnli'  # You can choose other versions of BERT like 'bert-large-uncased'
# model_name = 'M-FAC/bert-tiny-finetuned-qnli'
model_name = 'M-FAC/bert-mini-finetuned-qnli'
# model_name = 'gchhablani/bert-base-cased-finetuned-qnli'

# Load the tokenizer
tokenizer = BertTokenizer.from_pretrained(model_name)

# Load the BERT model
model = BertForSequenceClassification.from_pretrained(model_name)
max_length = 5

# ml.set_n_threads(24)

#
task_name = 'qnli'
dataset = load_dataset('glue', task_name)

# Access the evaluation dataset
validation = dataset['validation']

spdz_batch_size = 1
n_samples_to_run = 1
n_samples_to_run_pt = n_samples_to_run
compare_forward = True

# # # Load the MNLI dataset
# mnli_dataset = load_dataset('multi_nli')
# #
# # # Access the evaluation datasets
# mnli_validation_matched = mnli_dataset['validation_matched']
# mnli_validation_mismatched = mnli_dataset['validation_mismatched']

print("mnli_validation_matched", validation.shape)

task_to_keys = {
    "cola": ("sentence", None),
    "mnli": ("premise", "hypothesis"),
    "mrpc": ("sentence1", "sentence2"),
    "qnli": ("question", "sentence"),
    "qqp": ("question1", "question2"),
    "rte": ("sentence1", "sentence2"),
    "sst2": ("sentence", None),
    "stsb": ("sentence1", "sentence2"),
    "wnli": ("sentence1", "sentence2"),
}

# Function to tokenize the dataset
def tokenized_fn(example):
    sentence1_key, sentence2_key = task_to_keys[task_name]
    args = (
        (example[sentence1_key],) if sentence2_key is None else (example[sentence1_key], example[sentence2_key])
    )
    print("ARGS", args)
    encoded_input = tokenizer(*args, truncation=True, padding='max_length', max_length=max_length)
    print("ECODED", encoded_input)
    return encoded_input

def embed_fn(example):
    import numpy as np
    embedding_list = []
    # for input_id, token_type_ids in zip(example["input_ids"], example["token_type_ids"]):
    #     # embedding_list.append(torch.ones([8]))
    print(torch.tensor(example["input_ids"]))
    print("token_Type_ids", torch.tensor(example["token_type_ids"]))
    #     embedding = model.bert.embeddings(torch.tensor(input_id), token_type_ids=torch.tensor(token_type_ids)).detach()
    #     embedding_list.append(embedding)

    embedding = model.bert.embeddings(torch.tensor(example["input_ids"]), token_type_ids=torch.tensor(example["token_type_ids"])).detach()
    return { 'embedding': embedding }

# Tokenize the validation datasets
# with mnli_validation_matched.formatted_as("torch", ["input_ids", "token_type_ids"]):
tokenized_validation_matched_pt = validation.take(n_samples_to_run_pt).map(tokenized_fn, batched=True)
tokenized_validation = tokenized_validation_matched_pt.map(embed_fn, batched=True)
# with mnli_validation_mismatched.formatted_as("torch", ["input_ids", "token_type_ids"]):

print(tokenized_validation, type(tokenized_validation))

# get performance on PT model
def get_predictions(model, tokenized_dataset):
    model.eval()
    predictions = []
    criterion = torch.nn.BCEWithLogitsLoss()
    # with torch.no_grad():
    for i in range(n_samples_to_run_pt):
        example = tokenized_dataset[i]
        inputs = {key: torch.tensor([val]) for key, val in example.items() if key in ['input_ids', 'attention_mask', 'token_type_ids']}
        label = example['label']
        print("PT Inputs", inputs)
        outputs = model(**inputs)
        logits = outputs.logits
        # predicted_class = torch.argmax(logits, dim=-1).item()
        probabilities = torch.softmax(logits, dim=-1)
        predictions.append(probabilities.detach())

        label_onehot = torch.nn.functional.one_hot(torch.tensor([label]), num_classes=2)
        print("PT Label", label_onehot, label_onehot.dtype)
        loss = criterion(logits, label_onehot.float())
        print("PT Loss", loss)
        loss.backward()

    return predictions


def build_sfix_tensor(dataset):
    with dataset.formatted_as("torch", ["embedding", "label"]):
        tensor_embedding = torch.concat(list(map(lambda x: x['embedding'], dataset.iter(batch_size=1))))
        tensor_label = torch.concat(list(map(lambda x: x['label'], dataset.iter(batch_size=1))))

        # one-hot encode tensor_label
        tensor_label = torch.nn.functional.one_hot(tensor_label, num_classes=-1)

        tensor_embedding_sfix = sfix.input_tensor_via(0, tensor_embedding.numpy())
        tensor_label_sfix = sint.input_tensor_via(0, tensor_label.numpy())

        return tensor_embedding_sfix, tensor_label_sfix


test_x, test_y = build_sfix_tensor(tokenized_validation)
model_shape = test_x.shape
print("model shape", model_shape)
layers = ml.layers_from_torch(model, model_shape, input_via=0, batch_size=spdz_batch_size) # set batch size bigger?
optimizer = ml.SGD(layers)


def layers_for_bertlayer(bert_layer, bert_layer_pt):
    # Warning: Take care here that the order is in the way the hook is called
    return [
        (bert_layer.multi_head_attention, bert_layer_pt.attention),

        (bert_layer.intermediate, bert_layer_pt.intermediate),
        (bert_layer.output, bert_layer_pt.output),
        (bert_layer, bert_layer_pt),
    ]

layers_to_compare = [layers_for_bertlayer(l1, l2) for l1, l2 in zip(optimizer.layers[:-4], model.bert.encoder.layer)]
layers_to_compare = [x for xs in layers_to_compare for x in xs]
layers_to_compare.append((optimizer.layers[-4], model.bert.pooler))
# dropout
layers_to_compare.append((optimizer.layers[-2], model.classifier))

activation_list = []
def get_activation(name):
    def hook(model, input, output):
        activation_list.append((name, output[0].detach()))
    return hook

if compare_forward:
    relevant_pt_layers = model.bert.encoder.layer
else:
    relevant_pt_layers = model.bert.encoder.layer + [model.bert.pooler] + [model.classifier]

print(layers_to_compare)
for layer_id, (_, bert_layer) in enumerate(layers_to_compare):
    if compare_forward:
        bert_layer.register_forward_hook(get_activation(f'{layer_id}.{type(bert_layer)}'))
    else:
        bert_layer.register_full_backward_hook(get_activation(f'{layer_id}.{type(bert_layer)}'))

pt_predictions = get_predictions(model, tokenized_validation_matched_pt)
pt_predictions_top = torch.argmax(torch.cat(pt_predictions), dim=-1)
print_ln("PT Predictions %s", str(pt_predictions))
print_ln("PT Predictions Top %s", str(pt_predictions_top))
print("Running n_samples_to_run", n_samples_to_run)

print("Bert activations ", activation_list)



import numpy
def get_predictions(test_x, test_y):

    # print_ln("Embeddings %s", test_x.reveal_nested())

    # model_shape = [2] + test_x.shape[1:]

    optimizer.layers[-1].debug = True
    # optimizer.reset()
    # optimizer.print_random_update = True
    pt_predictions_tensor_spdz = sfix.input_tensor_via(0, numpy.array(numpy.concatenate(pt_predictions)))
    print_ln("PT Predictions %s %s", pt_predictions_tensor_spdz.reveal_nested(),  numpy.array(numpy.concatenate(pt_predictions)))

    # optimizer.layers[-1].Y.address = test_y.address
    # optimizer.layers[0].X.address = test_x.address
    # optimizer.run(batch_size=spdz_batch_size)

    #
    #
    print_ln("Test_x %s", test_x.reveal_nested())
    pred = optimizer.reveal_correctness(test_x, pt_predictions_tensor_spdz, batch_size=spdz_batch_size)
    print_ln("PRED %s", pred)
    #
    # compare the values in activation_list with the values in the layers

    # repeat to print all
    # repeated_layers = itertools.repeat(relevant_model_layers, n_samples_to_run)
    # repeated_layers_flat = itertools.chain(*repeated_layers)

    # reverse
    activ_list = activation_list
    if not compare_forward:
        activ_list = list(reversed(activ_list))

    for pt_values, (relevant_model_layer, _) in zip(activ_list, layers_to_compare):
        # compute diff
        pt_at_runtime = sfix.input_tensor_via(0, pt_values[1].numpy()).get_vector().reveal()
        if compare_forward:
            layer_output = relevant_model_layer.Y[0].get_vector().reveal()
        else:
            layer_output = relevant_model_layer.nabla_Y[0].get_vector().reveal()

        # display first 3 of both
        print_ln("pt_at_runtime %s (%s)", pt_at_runtime[:8], pt_values[0])
        print_ln("layer_output  %s", layer_output[:8])
        # print_ln("diff %s", [pt_at_runtime[i] - layer_output[i] for i in range(len(pt_at_runtime))])
        # compute diff
        diff = sum((pt_at_runtime - layer_output) ** 2)
        print_ln("diff %s", diff)

# last layer contains 0 in some dimensions, should match pretty closely since we only have dense



sfix.round_nearest = False
program.use_trunc_pr = False
# sfix.set_precision(18)
# cfix.set_precision(18)
# cfix.set_precision(f=32, k=62)
# sfix.set_precision(f=32, k=62)
# cfix.set_precision(f=14, k=31)
# sfix.set_precision(f=14, k=31)

print("Running tokenized_validation_matched")
get_predictions(test_x, test_y)

# print("Running tokenized_validation_mismatched")
# run(tokenized_validation_mismatched)
