

import torch
import numpy as np

# Set up the device (CUDA if available)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Random seed for reproducibility
torch.manual_seed(0)

# Example tensor (batch size, num_features)
# x = torch.randn(1, 128, requires_grad=True, device=device)
x = torch.tensor([[-0.846954, 1.88074, -1.84554, -2.81273, -1.20711, -1.21297, 0.62001, -0.141769, -0.949127, -1.73099, 1.16493, 0.540787, -0.190002, 0.882324, 0.728546, -1.69907, -1.86812, 0.553741, -2.46123, 0.762894, 0.33902, -0.0791321, 1.62549, 0.541946, 0.143051, -0.317627, -0.714722, -0.611969, -0.465164, -1.28044, 0.409103, -0.768906, -0.65593, 0.132355, -0.0928497, -1.06244, -0.343185, 0.886566, -1.0433, -0.912628, 1.35141, -1.28502, 2.84666, -0.0285797, 1.95341, -0.426666, 2.2648, -0.487747, 1.14572, -1.0081, 0.135742, 0.732513, -0.52153, 0.0297394, 1.68631, -2.05988, 0.692398, 0.458618, -1.66452, 0.669998, 1.46626, -1.65317, -0.789322, 0.390488, 0.307846, -0.0545959, -0.732742, -0.65773, 0.757767, -0.869644, -0.194656, -0.645081, -1.53149, -1.33018, -0.485931, 1.17586, 0.194611, 3.01472, 1.7811, -0.31102, 0.826797, 1.53583, -0.392548, 0.976471, -1.10039, -1.18486, 0.226624, -0.255325, 1.10582, -0.484039, 1.23885, 1.85809, -0.928177, 1.56113, 2.28717, 4.05235, -1.12653, 1.42021, 1.47694, -0.502136, -1.0645, 1.77956, 0.580154, 1.28958, 0.677872, -0.759842, -1.43552, -0.193054, -0.299789, -2.47662, -1.31201, -0.80043, 0.296234, 0.406525, -0.469543, 2.07043, -0.289078, 0.185944, 3.90395, -0.377258, -1.65337, 0.983002, 0.225143, -0.00578308, -0.828323, 0.931305, -0.822937, 0.706528]], requires_grad=True)

# Parameters for LayerNorm
gamma = torch.tensor([0.959686, 0.941589, 1.05974, 0.959763, 0.984146, 0.979874, 0.934647, 0.896622, 0.986786, 0.955017, 1.02715, 1.01804, 1.07263, 0.931702, 1.05113, 0.970169, 0.99202, 0.999496, 1.08017, 0.864349, 0.951157, 0.974655, 1.11697, 0.968719, 1.10092, 0.945709, 1.00319, 1.06441, 0.872955, 0.968124, 1.00693, 1.09352, 1.07964, 0.946335, 1.04408, 0.933838, 0.988159, 0.8741, 1.35799, 1.01859, 0.943298, 0.985809, 0.989868, 1.13182, 0.885742, 1.19781, 0.821762, 1.0471, 1.04845, 0.984711, 1.09901, 0.978455, 0.968704, 0.971542, 1.02397, 1.05675, 1.10599, 0.975342, 1.19577, 1.11389, 0.94783, 1.07253, 0.898376, 1.03561, 1.0269, 1.07295, 0.975876, 0.983963, 0.954956, 0.943848, 1.0766, 1.07527, 0.915497, 1.02397, 1.06187, 0.944458, 0.895416, 0.957962, 1.02887, 0.925766, 0.967636, 1.12428, 1.02351, 0.868622, 0.890213, 0.971405, 1.06602, 1.06076, 1.08368, 1.06381, 1.04706, 0.904221, 1.04602, 0.981903, 0.966156, 0.972931, 1.14618, 0.954422, 0.950516, 0.960983, 0.94577, 0.96431, 0.926147, 1.07387, 1.17513, 0.996964, 0.938202, 0.9711, 1.06361, 0.926971, 1.09271, 0.99054, 1.12662, 1.00473, 1.08553, 0.965103, 0.962143, 0.993439, 1.03954, 1.01836, 0.996445, 1.10217, 1.13878, 0.997574, 1.31555, 1.06331, 1.08102, 1.06195])
beta = torch.randn(128, requires_grad=True, device=device)
eps = 1e-5

# Create a LayerNorm layer
layer_norm = torch.nn.LayerNorm(x.size()[1:], elementwise_affine=True, eps=eps)
layer_norm.weight.data = gamma.clone()  # use the same initial gamma
layer_norm.bias.data = beta.clone()     # use the same initial beta

# Forward pass
y = layer_norm(x)

# Create gradients to backpropagate
# grad_y = torch.randn_like(y)
grad_y = torch.tensor(np.array([[0.000793457, -0.0062561, -0.00244141, 0.00283813, 0.0149078, 0.0100403, 0.00692749, 0.00120544, 0.000457764, -0.00561523, -0.000686646, -0.0080719, -0.00556946, 0.00411987, 0.000579834, -0.000228882, -0.000610352, -0.0085144, -0.00546265, 0.00338745, -0.00244141, 0.00454712, 0.00219727, 0.0042572, 0.000518799, -0.00184631, -0.00128174, -0.000534058, 0.0035553, 0.000244141, -0.0112457, 0.00779724, -0.00111389, -0.00164795, 0.0078125, 0.00695801, 0.00354004, 0.00300598, -0.000106812, -0.00273132, -0.00161743, 0.00624084, -0.00953674, -0.00050354, -0.00471497, -0.0022583, -0.0022583, 0.009552, 0.00604248, 0.00239563, 0.00300598, 0.00816345, -0.00192261, 0.00517273, -0.00704956, 0.00866699, -0.00410461, -0.00198364, 0.00822449, 0.00494385, 0.00350952, -0.00543213, -0.00950623, -0.00706482, -0.00746155, -0.00959778, -0.00335693, 0.00286865, -0.00761414, -0.00427246, 0.00320435, 0.00230408, -0.00234985, 0.00221252, -0.000167847, 0.00126648, 0.0139465, -0.00430298, -0.00311279, 0.00379944, 0.00344849, -0.00761414, -0.00456238, 0.00502014, -0.000152588, 0.00158691, -0.00195313, -0.0183716, 0.00163269, -0.00717163, 0.00480652, 0.000305176, 0.00050354, -0.00204468, -0.00648499, -0.00680542, 0.00582886, 0.00517273, 0.00317383, -0.00294495, -0.000137329, 0.000518799, 0.00881958, 0.00318909, -0.0105286, -0.00559998, -0.00801086, 0.00405884, 0.0119324, -0.00723267, -0.00590515, -0.0019989, -0.000701904, -0.00537109, 0.00909424, 0.0108185, -0.00440979, 0.00938416, 0.00265503, 0.0146332, 0.00512695, 0.00665283, 0.00683594, 0.00782776, -0.00752258, -0.0083313, -0.00744629, -0.00550842]]))

# Backward pass (PyTorch's autograd)
y.backward(grad_y)

# Display gradients computed by PyTorch
print("Gradients with respect to inputs (x):")
print(x.grad)
# print("Gradients with respect to gamma:")
# print(layer_norm.weight.grad)
# print("Gradients with respect to beta:")
# print(layer_norm.bias.grad)

# Manual computation for verification
mean = x.mean(dim=1, keepdim=True)
var = x.var(dim=1, keepdim=True, unbiased=False)
print(mean.shape)
x_normalized = (x - mean) / torch.sqrt(var + eps)
grad_gamma = (grad_y * x_normalized).sum(dim=0)
grad_beta = grad_y.sum(dim=0)

# Compute gradient wrt input manually
grad_input = grad_y * layer_norm.weight / torch.sqrt(var + eps)
print("x_normalized", x_normalized)
print("dnorm", grad_y * layer_norm.weight)
print("mean dnorm ", grad_input.mean(dim=1, keepdim=True) * torch.sqrt(var + eps), torch.sqrt(var + eps))
grad_input -= grad_input.mean(dim=1, keepdim=True)
print("pre", grad_input)

print((grad_input * x_normalized).mean(dim=1, keepdim=True))
grad_input -= x_normalized * (grad_input * x_normalized).mean(dim=1, keepdim=True)

# print("\nManually computed gradients with respect to gamma:")
# print(grad_gamma)
# print("Manually computed gradients with respect to beta:")
# print(grad_beta)
print("\nManually computed gradients with respect to input (x):")
print(grad_input)
